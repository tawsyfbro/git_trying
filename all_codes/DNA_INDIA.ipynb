{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "977f3898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains \n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ffe57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_news_links = []\n",
    "    \n",
    "url = 'https://www.dnaindia.com/search?q=bangladesh'\n",
    "\n",
    "\n",
    "base_url = \"https://www.firstpost.com/\"\n",
    "country = 'bangladesh'\n",
    "initial_url = f'{base_url}search?q={country}#gsc.tab=0&gsc.q=bangladesh&gsc.sort=date'\n",
    "\n",
    "# chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")\n",
    "# driver = webdriver.Chrome(options=chrome_options)\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "\n",
    "#this Website displays news articles with the help of Google/enchanced by Google.\n",
    "#So, each page needs to be separately selected and visited to scrape the links. \n",
    "#that can't be done by changing the Page Number in the URl. So, it's done using ActionsChains 'Click' function.\n",
    "\n",
    "#the links are then filtered to remove unnecessary news, since the search consists of any and all news that \n",
    "#remotely mention 'bangaldesh', even if it isn't strongly related to Bangladesh. \n",
    "\n",
    "#The first 5 pages are taken here, so 50 news links. That's because the news aren't sorted in order, and there are some news from \n",
    "#long ago that are present in the first page and some recent news present in later pages. \n",
    "#There are also multiple news links which lead to a 404 Error Page. \n",
    "\n",
    "for page in range(1,6):\n",
    "    \n",
    "    try:\n",
    "        buttons = driver.find_elements(By.CLASS_NAME, 'gsc-cursor-page')\n",
    "        \n",
    "        for button in buttons: \n",
    "           \n",
    "            if button.get_attribute('innerHTML') == f'{page}':\n",
    "                print(f'in page {page}')\n",
    "                ActionChains(driver)\\\n",
    "                .click(button)\\\n",
    "                .perform()\n",
    "                time.sleep(20)                  \n",
    "\n",
    "                main_div = driver.find_element(By.XPATH, '/html/body/div[2]/div/div[2]/div/div[2]/div/div/div/div/div[5]/div[2]/div[1]/div/div[1]')\n",
    "     \n",
    "                if main_div:\n",
    "                    print('main_div found')\n",
    "                    time.sleep(20)\n",
    "                    news = main_div.find_elements(By.CSS_SELECTOR, 'a')\n",
    "\n",
    "                    for n in news:\n",
    "                        news_text = n.get_attribute('data-ctorig')\n",
    "\n",
    "                        if news_text in all_news_links:\n",
    "                            continue\n",
    "                        else:\n",
    "                            if 'bangladesh' in news_text or 'bangladeshs' in news_text:              \n",
    "                                print(news_text)\n",
    "                                all_news_links.append(news_text)\n",
    "                            else:\n",
    "                                continue     \n",
    "    except:\n",
    "       \n",
    "            pass\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd75ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_news_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38fa78f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in page 1\n",
      "main_div found\n",
      "in page 2\n",
      "main_div found\n",
      "0\n",
      "1\n",
      "Skipped due to missing info.\n",
      "1\n",
      "2\n",
      "Skipped due to missing info.\n",
      "2\n",
      "3\n",
      "Skipped due to missing info.\n",
      "3\n",
      "4\n",
      "Skipped due to missing info.\n",
      "4\n",
      "5\n",
      "Skipped due to missing info.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 74\u001b[0m\n\u001b[0;32m     72\u001b[0m driver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome(options\u001b[38;5;241m=\u001b[39mchrome_options)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m#driver = webdriver.Chrome()\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(link)\n\u001b[0;32m     76\u001b[0m country \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIndia\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:357\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    356\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mGET, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: url})\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:346\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m    344\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[1;32m--> 346\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:300\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    298\u001b[0m data \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mdump_json(params)\n\u001b[0;32m    299\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(command_info[\u001b[38;5;241m0\u001b[39m], url, body\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:321\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    318\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[1;32m--> 321\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn\u001b[38;5;241m.\u001b[39mrequest(method, url, body\u001b[38;5;241m=\u001b[39mbody, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m    322\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\request.py:78\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[1;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[0;32m     75\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[0;32m     76\u001b[0m     )\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_body(\n\u001b[0;32m     79\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[0;32m     80\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\request.py:170\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mupdate(headers)\n\u001b[0;32m    168\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[1;32m--> 170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\poolmanager.py:376\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    374\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 376\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, u\u001b[38;5;241m.\u001b[39mrequest_uri, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    378\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    715\u001b[0m     conn,\n\u001b[0;32m    716\u001b[0m     method,\n\u001b[0;32m    717\u001b[0m     url,\n\u001b[0;32m    718\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    719\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    720\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    721\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    722\u001b[0m )\n\u001b[0;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[0;32m    728\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    461\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    462\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m             six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 461\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1378\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_news_links = []\n",
    "\n",
    "base_url = \"https://www.dnaindia.com/\"\n",
    "country = 'bangladesh'\n",
    "initial_url = f'{base_url}search?q={country}'\n",
    "\n",
    "# chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")\n",
    "# driver = webdriver.Chrome(options=chrome_options)\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(initial_url)\n",
    "\n",
    "#this Website displays news articles with the help of Google/enchanced by Google.\n",
    "#So, each page needs to be separately selected and visited to scrape the links. \n",
    "#that can't be done by changing the Page Number in the URl. So, it's done using ActionsChains 'Click' function.\n",
    "\n",
    "#the links are then filtered to remove unnecessary news, since the search consists of any and all news that \n",
    "#remotely mention 'bangaldesh', even if it isn't strongly related to Bangladesh. \n",
    "\n",
    "#The first 5 pages are taken here, so 50 news links. That's because the news aren't sorted in order, and there are some news from \n",
    "#long ago that are present in the first page and some recent news present in later pages. \n",
    "#There are also multiple news links which lead to a 404 Error Page. \n",
    "\n",
    "#WHILE LOOP CAN BE USED FOR FINDING TOTAL NUMBER OF PAGES. 2 PAGES TAKEN HERE \n",
    "\n",
    "for page in range(1,3):\n",
    "    \n",
    "    try:\n",
    "        buttons = driver.find_elements(By.CLASS_NAME, 'gsc-cursor-page')\n",
    "        \n",
    "        for button in buttons: \n",
    "           \n",
    "            if button.get_attribute('innerHTML') == f'{page}':\n",
    "                print(f'in page {page}')\n",
    "                ActionChains(driver)\\\n",
    "                .click(button)\\\n",
    "                .perform()\n",
    "                time.sleep(20)                  \n",
    "\n",
    "                main_div = driver.find_element(By.XPATH, '/html/body/div[2]/div/div[2]/div/div[2]/div/div/div/div/div[5]/div[2]/div[1]/div/div[1]')\n",
    "     \n",
    "                if main_div:\n",
    "                    print('main_div found')\n",
    "                    time.sleep(10)\n",
    "                    news = main_div.find_elements(By.CSS_SELECTOR, 'a')\n",
    "\n",
    "                    for n in news:\n",
    "                        news_text = n.get_attribute('data-ctorig')\n",
    "\n",
    "                        if news_text in all_news_links:\n",
    "                            continue\n",
    "                        else:\n",
    "                            if 'bangladesh' in news_text or 'bangladeshs' in news_text:              \n",
    "#                                 print(news_text)\n",
    "                                all_news_links.append(news_text)\n",
    "                            else:\n",
    "                                continue     \n",
    "    except:\n",
    "       \n",
    "            pass\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "\n",
    "data_list = []\n",
    "counter = 0\n",
    "for link in all_news_links:\n",
    "    \n",
    "\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    #driver = webdriver.Chrome()\n",
    "    driver.get(link)\n",
    "\n",
    "    country = 'India'\n",
    "\n",
    "    try:\n",
    "\n",
    "        title_tag = driver.find_element(By.CLASS_NAME, 'article-heading')\n",
    "        title = title_tag.get_attribute('innerHTML')\n",
    "    except:\n",
    "        title = 'Title Not Found'\n",
    "\n",
    "\n",
    "    try:\n",
    "        author_main_tag = driver.find_element(By.ID, 'dna-home')\n",
    "        author_tag = author_main_tag.find_element(By.CSS_SELECTOR, 'a')\n",
    "        author = author_tag.get_attribute('innerHTML')\n",
    "\n",
    "    except:\n",
    "        author = 'Author Not Found'\n",
    "\n",
    "    #The Date Info is available in different formats for different news articles. It's a bit difficult to match format for all\n",
    "    #articles. But, the functions below should match most articles.\n",
    "    #I was not able to find the Date Info in the Meta Tags. \n",
    "    \n",
    "    #initializing here. if these stay blank, the news link will be automatically eliminated.\n",
    "    \n",
    "    source_localtime = ''\n",
    "    bangladesh_localtime = ''\n",
    "    \n",
    "    try:  \n",
    "        date_main_tag = driver.find_element(By.CLASS_NAME, 'dna-update')\n",
    "        date_data = date_main_tag.get_attribute('innerHTML')\n",
    "    \n",
    "    except:\n",
    "                \n",
    "        try:\n",
    "            date_main_tag = driver.find_element(By.CLASS_NAME, 'article-author-txt')\n",
    "            date_data = date_main_tag.get_attribute('innerHTML').split('Updated: ')[1]\n",
    "            \n",
    "        except NoSuchElementException:\n",
    "            date_data = 'Date Data Not Found'\n",
    "\n",
    "    if date_data!='Date Data not found':\n",
    "        \n",
    "        if '>' in date_data:\n",
    "            \n",
    "            only_date = date_data.split('>')[1]\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            only_date = date_data\n",
    "\n",
    "        try:\n",
    "            \n",
    "#             format_change = datetime.strptime(only_date,'%b %d, %Y, %I:%M %p IST') + timedelta(minutes = 30)\n",
    "            \n",
    "            source_localtime = datetime.strptime(only_date, \"%b %d, %Y, %I:%M %p IST\")\n",
    "            bangladesh_localtime = source_localtime + timedelta(minutes=30)\n",
    "            \n",
    "\n",
    "        \n",
    "        except ValueError:\n",
    "            \n",
    "            date_data = 'Date Data Not Found'    \n",
    "\n",
    "\n",
    "    #CHAGING TO BEAUTIFUL SOUP \n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    content_div = soup.find('div', class_='article-description')\n",
    "\n",
    "    if content_div:\n",
    "    \n",
    "    #the content is inside <p> tags, and in some websites, they are also inside <ul> tags. \n",
    "    #so, here, we try to fin all <p> and <ul> tags. However, not all <ul> tags represent actual content.\n",
    "    #The <ul> tags that represent News Content don't have any particular Class Name, so we're taking all <ul> content\n",
    "    #that don't have a class name. It has been working correctly so far. \n",
    "    \n",
    "        paragraph_tags = content_div.find_all(True)\n",
    "        content = []\n",
    "    \n",
    "        for tag in paragraph_tags:\n",
    "\n",
    "            if tag.name == 'p':\n",
    "\n",
    "                content.append(tag.text)\n",
    "\n",
    "            elif tag.name == 'ul':        \n",
    "                if not tag.has_attr('class'):\n",
    "                    content.append(tag.text)\n",
    "            else:\n",
    "                continue \n",
    "\n",
    "        full_content = ' '.join(content)\n",
    "        full_content = full_content.split('READ')[0]\n",
    "        \n",
    "    else:\n",
    "        full_content = 'Content Not Found'\n",
    "\n",
    "\n",
    "    content_summary_section = soup.find('section',class_ ='article-details')\n",
    "    \n",
    "    if content_summary_section:\n",
    "        \n",
    "        content_summary_tag = content_summary_section.find('p', class_ = 'article-short-desc')\n",
    "        content_summary = content_summary_tag.text if content_summary_tag else 'Content summary not found'\n",
    "        \n",
    "    title_translation = 'None'\n",
    "    summary_translation = 'None'\n",
    "    content_translation = 'None'\n",
    "\n",
    "#     data_dict = {\n",
    "#         \"country\": country,\n",
    "#         \"author\": author,\n",
    "#         \"url\": link,\n",
    "#         \"title\": title,\n",
    "#         \"timestamp\": timestamp,\n",
    "#         \"date\": date,\n",
    "#         \"date_str\": date_str,\n",
    "#         \"content\": full_content,\n",
    "#         \"content_summary\": content_summary\n",
    "#     }\n",
    "    \n",
    "    data_dict = {\n",
    "            \"url\": link,\n",
    "            \"title\": title,\n",
    "            \"content\": full_content,\n",
    "            \"content_summary\": content_summary,\n",
    "            \"title_translation\":title_translation,\n",
    "            \"content_translation\":content_translation,\n",
    "            \"summary translation\":summary_translation,\n",
    "            \"author\": author,\n",
    "            \"country\": country,\n",
    "            'source_localtime': source_localtime,\n",
    "            'bangladesh_localtime': bangladesh_localtime\n",
    "\n",
    "        }\n",
    "    \n",
    "    \n",
    "    print(counter)\n",
    "    counter+=1\n",
    "\n",
    "\n",
    "    if (date_data != \"Date Data Not Found\" and full_content != \"Content Not Found\" and content_summary != \"Content summary not found\"):\n",
    "        if data_dict not in data_list:\n",
    "            # Adding to data list\n",
    "            data_list.append(data_dict)\n",
    "            print(f'Link {counter} added')\n",
    "    else:\n",
    "        print(counter)\n",
    "        print('Skipped due to missing info.')\n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075b39b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'India'\n",
    "csv_filename = f\"{country}_DNA_India.csv\"\n",
    "\n",
    "# Checking if the CSV file already exists\n",
    "if os.path.exists(csv_filename):\n",
    "    existing_df = pd.read_csv(csv_filename)\n",
    "    # Merging new and existing dataframe\n",
    "    df = pd.concat([existing_df, pd.DataFrame(data_list)], ignore_index=True)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])  # Converting the \"date\" column to datetime\n",
    "    df = df.drop_duplicates(subset=[\"title\"], keep=\"first\")\n",
    "    df = df.sort_values(by=\"date\", ascending=False)  # Sorting the date\n",
    "    df = df.reset_index(drop=True)\n",
    "    df.to_csv(csv_filename, index=False)  \n",
    "else:\n",
    "    # If csv file does not exist, then we create a new CSV file with the scraped data\n",
    "    df = pd.DataFrame(data_list)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], format = \"%d-%m-%Y\")  # Converting the \"date\" column to datetime\n",
    "    df = df.sort_values(by=\"date\", ascending=False)\n",
    "    df = df.drop_duplicates(subset=[\"title\"], keep=\"first\")\n",
    "    df = df.reset_index(drop=True)\n",
    "    df.to_csv(csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3da32745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.dnaindia.com/analysis/report-dna-tv-show-why-were-elections-in-bangladesh-controversial-this-year-awami-league-sheikh-hasina-sourabh-jain-3073910',\n",
       " 'https://www.dnaindia.com/explainer/report-dna-explainer-what-is-the-water-dispute-between-india-and-bangladesh-know-purpose-of-kushiyara-river-pact-2983049',\n",
       " 'https://www.dnaindia.com/cricket/report-meet-umme-ahmed-shishir-wife-of-bangladesh-captain-shakib-al-hasan-know-their-beautiful-love-story-3066787',\n",
       " 'https://www.dnaindia.com/sports/report-rare-incident-on-cricket-field-bangladesh-recalls-ish-sodhi-after-controversial-runout-hasan-mahmud-liton-das-3062042',\n",
       " 'https://www.dnaindia.com/cricket/bangladesh-vs-sri-lanka-live-score-world-cup-2023-eliminated-bangladesh-playing-for-pride-against-sri-lanka-3067325',\n",
       " 'https://www.dnaindia.com/cricket/report-champions-trophy-2025-qualification-scenarios-for-bangladesh-netherlands-3068110',\n",
       " 'https://www.dnaindia.com/world/report-bangladesh-polls-sheikh-hasina-s-party-manifesto-pledges-ongoing-cooperation-friendly-ties-with-india-3072845',\n",
       " 'https://www.dnaindia.com/world/video-bangladesh-elections-2024-amid-tight-security-pm-sheikh-hasina-to-cruise-to-5th-victory-unopposed-3073772',\n",
       " 'https://www.dnaindia.com/world/report-15-killed-several-injured-after-two-trains-collide-in-bangladesh-bhairab-upazila-kishoreganj-3065675',\n",
       " 'https://www.dnaindia.com/sports/report-bangladesh-clinch-maiden-u19-asia-cup-title-after-defeating-uae-by-195-runs-3071873',\n",
       " 'https://www.dnaindia.com/explainer/report-dna-explainer-after-maldives-india-out-campaign-gains-traction-in-bangladesh-here-s-why-3077525',\n",
       " 'https://www.dnaindia.com/cricket/report-sa-vs-ban-south-africa-defeats-bangladesh-by-149-runs-climbs-to-2nd-spot-3065776',\n",
       " 'https://www.dnaindia.com/sports/report-ban-vs-sl-world-cup-2023-shakib-shanto-shine-as-bangladesh-beat-sri-lanka-by-3-wickets-angelo-mathews-3067422',\n",
       " 'https://www.dnaindia.com/sports/report-icc-champions-trophy-2025-england-bangladesh-likely-to-miss-the-marquee-tournament-know-why-3066289',\n",
       " 'https://www.dnaindia.com/cricket/report-big-boost-for-bangladesh-as-litton-das-returns-to-squad-for-asia-cup-super-4-stage-3058968',\n",
       " 'https://www.dnaindia.com/cricket/report-pak-vs-ban-pakistan-knock-bangladesh-out-of-world-cup-2023-with-seven-wicket-win-3066577',\n",
       " 'https://www.dnaindia.com/cricket/report-bangladesh-record-biggest-test-victory-of-21st-century-crush-afghanistan-by-546-runs-in-one-off-test-3048030',\n",
       " 'https://www.dnaindia.com/lifestyle/report-eid-ul-fitr-moon-sighting-2023-live-updates-moon-sighting-in-india-pakistan-bangladesh-tonight-eid-ka-chand-3037764',\n",
       " 'https://www.dnaindia.com/cricket/report-aus-vs-ban-mitchell-marsh-adam-zampa-shine-as-australia-beat-bangladesh-by-8-wickets-3068016',\n",
       " 'https://www.dnaindia.com/cricket/report-india-vs-bangladesh-icc-women-s-t20i-world-cup-2020-live-streaming-teams-time-in-india-ist-where-to-watch-on-tv-2814751']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_news_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "085a5d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 30\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m     alt_tag \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle-author-txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m     alt_date \u001b[38;5;241m=\u001b[39m alt_tag\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUpdated: \u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     32\u001b[0m     remove_IST_and_others \u001b[38;5;241m=\u001b[39m alt_date\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m IST\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(remove_IST_and_others) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "url = 'https://www.dnaindia.com/cricket/bangladesh-vs-sri-lanka-live-score-world-cup-2023-eliminated-bangladesh-playing-for-pride-against-sri-lanka-3067325'\n",
    "url_2 = 'https://www.dnaindia.com/explainer/report-dna-explainer-what-is-the-water-dispute-between-india-and-bangladesh-know-purpose-of-kushiyara-river-pact-2983049'\n",
    "# for link in all_news_links[:15]:\n",
    "    \n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "print(response.status_code)\n",
    "\n",
    "date_main_tag = soup.find('p', class_ = 'dna-update')\n",
    "\n",
    "if date_main_tag:\n",
    "    \n",
    "    date_data = date_main_tag.text\n",
    "    date_data = date_data.split(': ',1)[1]\n",
    "\n",
    "    remove_IST_and_others = date_data.split(' IST | ')\n",
    "\n",
    "    if len(remove_IST_and_others) > 1:\n",
    "\n",
    "        date_data = remove_IST_and_others[0]\n",
    "\n",
    "    else:\n",
    "\n",
    "        date_data = remove_IST_and_others[0].split(' IST ')[0]\n",
    "        \n",
    "else:\n",
    "    \n",
    "    alt_tag = soup.find('div', class_ = 'article-author-txt')\n",
    "    \n",
    "    alt_date = alt_tag.text.split('Updated: ')[1]\n",
    "    \n",
    "    remove_IST_and_others = alt_date.split(' IST')\n",
    "\n",
    "    if len(remove_IST_and_others) > 1:\n",
    "\n",
    "        date_data = remove_IST_and_others[0]\n",
    "\n",
    "    else:\n",
    "\n",
    "        date_data = remove_IST_and_others[0].split(' IST ')[0]\n",
    "\n",
    "\n",
    "date_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe1bbca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nov 06, 2023, 10:01 PM IST'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 'Reported By:| Edited By: DNA  Web Team |Source:  |Updated: Nov 06, 2023, 10:01 PM IST'\n",
    "\n",
    "x.split('Updated: ')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e8266e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

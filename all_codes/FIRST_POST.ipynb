{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f395be27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains \n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c767159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_news_links = []\n",
    "    \n",
    "url = 'https://www.firstpost.com/search?q=bangladesh#gsc.tab=0&gsc.q=bangladesh&gsc.sort=date'\n",
    "\n",
    "\n",
    "base_url = \"https://www.firstpost.com/\"\n",
    "country = 'bangladesh'\n",
    "initial_url = f'{base_url}search?q={country}#gsc.tab=0&gsc.q=bangladesh&gsc.sort=date'\n",
    "\n",
    "# chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")\n",
    "# driver = webdriver.Chrome(options=chrome_options)\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(initial_url)\n",
    "\n",
    "#this Website displays news articles with the help of Google/enchanced by Google.\n",
    "#So, each page needs to be separately selected and visited to scrape the links. \n",
    "#that can't be done by changing the Page Number in the URl. So, it's done using ActionsChains 'Click' function.\n",
    "\n",
    "#the links are then filtered to remove unnecessary news, since the search consists of any and all news that \n",
    "#remotely mention 'bangaldesh', even if it isn't strongly related to Bangladesh. \n",
    "#The news that only shows Scorecard of Cricket Matches are also filtered out. \n",
    "\n",
    "page =1 \n",
    "# for page in range(1,4):\n",
    "\n",
    "while True:\n",
    "    \n",
    "    try:\n",
    "        buttons = driver.find_elements(By.CLASS_NAME, 'gsc-cursor-page')\n",
    "        for button in buttons: \n",
    "        \n",
    "            \n",
    "            if button.get_attribute('innerHTML') == f'{page}':\n",
    "\n",
    "                ActionChains(driver)\\\n",
    "                .click(button)\\\n",
    "                .perform()\n",
    "                time.sleep(10)                  \n",
    "\n",
    "                main_div = driver.find_element(By.XPATH, '/html/body/div[1]/div[3]/div[2]/div/div/div/div/div/div/div[5]/div[2]/div[2]/div/div[1]')\n",
    "\n",
    "\n",
    "                if main_div:\n",
    "                    news = main_div.find_elements(By.CSS_SELECTOR, 'a')\n",
    "\n",
    "                    for n in news:\n",
    "                        news_text = n.get_attribute('data-ctorig')\n",
    "\n",
    "                        if news_text in all_news_links:\n",
    "                            continue\n",
    "                        else:\n",
    "                            if 'bangladesh' in news_text or 'bangladeshs' in news_text:\n",
    "                                if 'cricket-live-score' in news_text:\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    all_news_links.append(news_text)\n",
    "                            else:\n",
    "                                continue     \n",
    "    except:\n",
    "       \n",
    "            pass\n",
    "    \n",
    "    page+=1\n",
    "    \n",
    "    if page == 4:\n",
    "        \n",
    "        break\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc9e1101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_news_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fff183c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_news_links = []\n",
    "    \n",
    "url = 'https://www.firstpost.com/search?q=bangladesh#gsc.tab=0&gsc.q=bangladesh&gsc.sort=date'\n",
    "\n",
    "\n",
    "base_url = \"https://www.firstpost.com/\"\n",
    "country = 'bangladesh'\n",
    "initial_url = f'{base_url}search?q={country}#gsc.tab=0&gsc.q=bangladesh&gsc.sort=date'\n",
    "\n",
    "# chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")\n",
    "# driver = webdriver.Chrome(options=chrome_options)\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "\n",
    "#this Website displays news articles with the help of Google/enchanced by Google.\n",
    "#So, each page needs to be separately selected and visited to scrape the links. \n",
    "#that can't be done by changing the Page Number in the URl. So, it's done using ActionsChains 'Click' function.\n",
    "\n",
    "#the links are then filtered to remove unnecessary news, since the search consists of any and all news that \n",
    "#remotely mention 'bangaldesh', even if it isn't strongly related to Bangladesh. \n",
    "#The news that only shows Scorecard of Cricket Matches are also filtered out. \n",
    "\n",
    "for page in range(1,3):\n",
    "    \n",
    "    try:\n",
    "        buttons = driver.find_elements(By.CLASS_NAME, 'gsc-cursor-page')\n",
    "        for button in buttons: \n",
    "        \n",
    "            \n",
    "            if button.get_attribute('innerHTML') == f'{page}':\n",
    "\n",
    "                ActionChains(driver)\\\n",
    "                .click(button)\\\n",
    "                .perform()\n",
    "                time.sleep(10)                  \n",
    "\n",
    "                main_div = driver.find_element(By.XPATH, '/html/body/div[1]/div[3]/div[2]/div/div/div/div/div/div/div[5]/div[2]/div[2]/div/div[1]')\n",
    "\n",
    "\n",
    "                if main_div:\n",
    "                    news = main_div.find_elements(By.CSS_SELECTOR, 'a')\n",
    "\n",
    "                    for n in news:\n",
    "                        news_text = n.get_attribute('data-ctorig')\n",
    "\n",
    "                        if news_text in all_news_links:\n",
    "                            continue\n",
    "                        else:\n",
    "                            if 'bangladesh' in news_text or 'bangladeshs' in news_text:\n",
    "                                if 'cricket-live-score' in news_text:\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    all_news_links.append(news_text)\n",
    "                            else:\n",
    "                                continue     \n",
    "    except:\n",
    "       \n",
    "            pass\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "counter = 0\n",
    "data_list = []\n",
    "\n",
    "for url in all_news_links:\n",
    "    \n",
    "    country = 'india'\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "#     driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    \n",
    "    try:\n",
    "        title_tag = driver.find_element(By.CLASS_NAME, 'inner-main-title')\n",
    "        if title_tag:\n",
    "            title = title_tag.get_attribute('innerHTML').strip()\n",
    "        else:\n",
    "            title = 'Title not found'\n",
    "            \n",
    "    except Exception:\n",
    "        \n",
    "        title_tag = driver.find_element(By.CLASS_NAME, 'article-title')\n",
    "        if title_tag:\n",
    "            title = title_tag.get_attribute('innerHTML').strip()\n",
    "        else:\n",
    "            title = 'Title not found'\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        print(counter)\n",
    "        title = 'Title not found'\n",
    "\n",
    "    date_tags = driver.find_elements(By.TAG_NAME, \"meta\")\n",
    "\n",
    "    for meta_tag in date_tags:\n",
    "        if meta_tag.get_attribute(\"property\") == \"article:modified_time\":\n",
    "            date_data = meta_tag.get_attribute(\"content\")\n",
    "            break\n",
    "        elif meta_tag.get_attribute(\"property\") == \"article:published_time\":\n",
    "            date_data = meta_tag.get_attribute(\"content\")\n",
    "            break\n",
    "\n",
    "    only_date = date_data.split('T')[0]\n",
    "    only_time = date_data.split('T')[1]\n",
    "    time = f\"{only_time.split(':')[0]}:{only_time.split(':')[1]}\"\n",
    "    cleaned_date = f\"{only_date},{time}\"\n",
    "\n",
    "    source_localtime = datetime.strptime(cleaned_date, \"%Y-%m-%d,%H:%M\")\n",
    "    bangladesh_localtime = source_localtime + timedelta(minutes=30)\n",
    "\n",
    "    p_lists = []\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        content_div = driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div[3]/div/div[1]/div[2]/div[4]')\n",
    "    \n",
    "        paragraphs = content_div.find_elements(By.CSS_SELECTOR,'p')\n",
    "\n",
    "        for paras in paragraphs:\n",
    "\n",
    "            p_lists.append(paras.get_attribute('innerHTML'))\n",
    "    \n",
    "    except Exception:\n",
    "        \n",
    "        content_div = driver.find_element(By.CLASS_NAME, 'inner-copy')\n",
    "        \n",
    "        paragraphs = content_div.find_elements(By.CSS_SELECTOR,'p')\n",
    "\n",
    "        for paras in paragraphs:\n",
    "\n",
    "            p_lists.append(paras.get_attribute('innerHTML'))\n",
    "            \n",
    "    except:\n",
    "        \n",
    "        print(f'content,{counter}')\n",
    "        p_lists = ['Content Not Found']\n",
    "        \n",
    "\n",
    "    full_content = ' '.join(p_lists)\n",
    "\n",
    "\n",
    "    #This is to remove the news after (With agency inputs) in the content, if present.\n",
    "    temp_cuts = full_content.split('(With agency inputs)', 1)\n",
    "\n",
    "    full_content = temp_cuts[0].strip()\n",
    "\n",
    "    #This is to remove the news after LATEST NEWS in the content if present.\n",
    "    temp_cuts = full_content.split('Latest News', 1)\n",
    "\n",
    "    full_content = temp_cuts[0].strip()\n",
    "    \n",
    "    #This is to remove the HTML TAGS in the content if present.\n",
    "    full_content = re.sub('<strong>|</strong>|<em>|</em>','',full_content)\n",
    "\n",
    "    try:    \n",
    "        content_summary_tag = driver.find_element(By.CLASS_NAME, 'inner-copy-excerpt')\n",
    "        content_summary = (content_summary_tag.get_attribute('innerHTML')).strip()\n",
    "            \n",
    "    except Exception:\n",
    "        \n",
    "        content_summary_tag = driver.find_element(By.CLASS_NAME, 'inner-copy')\n",
    "        content_summary = (content_summary_tag.get_attribute('innerHTML')).strip()\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        print(f'summary,{counter}')\n",
    "        content_summary = 'Content summary not found'\n",
    "        \n",
    "    title_translation = 'None'\n",
    "    summary_translation = 'None'\n",
    "    content_translation = 'None'\n",
    "    \n",
    "#     data_dict = {\n",
    "#         \"country\": country,\n",
    "#         \"url\": url,\n",
    "#         \"title\": title,\n",
    "#         \"timestamp\": timestamp,\n",
    "#         \"date\": date,\n",
    "#         \"date_str\": date_str,\n",
    "#         \"content\": full_content,\n",
    "#         \"content_summary\": content_summary\n",
    "#     }\n",
    "    \n",
    "    data_dict = {\n",
    "            \"url\": url,\n",
    "            \"title\": title,\n",
    "            \"content\": full_content,\n",
    "            \"content_summary\": content_summary,\n",
    "            \"title_translation\":title_translation,\n",
    "            \"content_translation\":content_translation,\n",
    "            \"summary translation\":summary_translation,\n",
    "            \"author\": author,\n",
    "            \"country\": country,\n",
    "            'source_localtime': source_localtime,\n",
    "            'bangladesh_localtime': bangladesh_localtime\n",
    "\n",
    "        }\n",
    "\n",
    "    counter+=1\n",
    "\n",
    "\n",
    "    if (title != \"Title not found\" and full_content != \"Content Not Found\" and content_summary != \"Content summary not found\"):\n",
    "        if data_dict not in data_list:\n",
    "            # Adding to data list\n",
    "            data_list.append(data_dict)\n",
    "            print(f'Link {counter} added')\n",
    "    else:\n",
    "        print(counter)\n",
    "        print('Skipped due to missing info.')\n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e553cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'India'\n",
    "csv_filename = f\"{country}_First_Post.csv\"\n",
    "\n",
    "# Checking if the CSV file already exists\n",
    "if os.path.exists(csv_filename):\n",
    "    existing_df = pd.read_csv(csv_filename)\n",
    "    # Merging new and existing dataframe\n",
    "    df = pd.concat([existing_df, pd.DataFrame(data_list)], ignore_index=True)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])  # Converting the \"date\" column to datetime\n",
    "    df = df.drop_duplicates(subset=[\"title\"], keep=\"first\")\n",
    "    df = df.sort_values(by=\"date\", ascending=False)  # Sorting the date\n",
    "    df = df.reset_index(drop=True)\n",
    "    df.to_csv(csv_filename, index=False)  \n",
    "else:\n",
    "    # If csv file does not exist, then we create a new CSV file with the scraped data\n",
    "    df = pd.DataFrame(data_list)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], format = \"%d-%m-%Y\")  # Converting the \"date\" column to datetime\n",
    "    df = df.sort_values(by=\"date\", ascending=False)\n",
    "    df = df.drop_duplicates(subset=[\"title\"], keep=\"first\")\n",
    "    df = df.reset_index(drop=True)\n",
    "    df.to_csv(csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42624f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

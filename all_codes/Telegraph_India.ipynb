{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c2164057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pages taken is 13\n",
      "Link 1 added\n",
      "Link 2 added\n",
      "Link 3 added\n",
      "Link 4 added\n",
      "Link 5 added\n",
      "Link 6 added\n",
      "Link 7 added\n",
      "Link 8 added\n",
      "Link 9 added\n",
      "Link 10 added\n",
      "Link 11 added\n",
      "Link 12 added\n",
      "Link 13 added\n",
      "Link 14 added\n",
      "Link 15 added\n",
      "Link 16 added\n",
      "Link 17 added\n",
      "Link 18 added\n",
      "Link 19 added\n",
      "Link 20 added\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>content_summary</th>\n",
       "      <th>title_translation</th>\n",
       "      <th>content_translation</th>\n",
       "      <th>summary translation</th>\n",
       "      <th>author</th>\n",
       "      <th>country</th>\n",
       "      <th>source_localtime</th>\n",
       "      <th>bangladesh_localtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.telegraphindia.com//north-east/ban...</td>\n",
       "      <td>Bangladesh visa centre in Assam's Silchar soon...</td>\n",
       "      <td>A Bangladesh visa centre will be opened here s...</td>\n",
       "      <td>'A proposal to start a border market at Harina...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>PTI</td>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>2024-02-19 13:58:00</td>\n",
       "      <td>2024-02-19 14:28:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.telegraphindia.com//west-bengal/ea...</td>\n",
       "      <td>Eastern Himalaya Travel &amp; Tour Operators’ Asso...</td>\n",
       "      <td>The Eastern Himalaya Travel &amp; Tour Operators’ ...</td>\n",
       "      <td>Footfall of tourists from Bangladesh had incre...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Avijit Sinha</td>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>2024-02-19 11:04:00</td>\n",
       "      <td>2024-02-19 11:34:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.telegraphindia.com//gallery/news-o...</td>\n",
       "      <td>News of the day: Three Bengal ministers visit ...</td>\n",
       "      <td>The Eastern Himalaya Travel &amp; Tour Operators’ ...</td>\n",
       "      <td>Here are the latest developments from India an...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Our Web Desk</td>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>2024-02-18 17:34:00</td>\n",
       "      <td>2024-02-18 18:04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.telegraphindia.com//sports/cricket...</td>\n",
       "      <td>Bangladesh pacer Mustafizur Rahman hospitalise...</td>\n",
       "      <td>Experienced left-arm pacer Mustafizur Rahman w...</td>\n",
       "      <td>The accident occurred while they were particip...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>PTI</td>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>2024-02-18 15:48:00</td>\n",
       "      <td>2024-02-18 16:18:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.telegraphindia.com//my-kolkata/eve...</td>\n",
       "      <td>10th Indo-Bangla Cross Border International Cy...</td>\n",
       "      <td>On February 15, 13 cyclists began their journe...</td>\n",
       "      <td>Thirteen cyclists will travel 350km over seven...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Debrup Chaudhuri</td>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>2024-02-17 15:40:00</td>\n",
       "      <td>2024-02-17 16:10:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.telegraphindia.com//north-east/ban...   \n",
       "1  https://www.telegraphindia.com//west-bengal/ea...   \n",
       "2  https://www.telegraphindia.com//gallery/news-o...   \n",
       "3  https://www.telegraphindia.com//sports/cricket...   \n",
       "4  https://www.telegraphindia.com//my-kolkata/eve...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Bangladesh visa centre in Assam's Silchar soon...   \n",
       "1  Eastern Himalaya Travel & Tour Operators’ Asso...   \n",
       "2  News of the day: Three Bengal ministers visit ...   \n",
       "3  Bangladesh pacer Mustafizur Rahman hospitalise...   \n",
       "4  10th Indo-Bangla Cross Border International Cy...   \n",
       "\n",
       "                                             content  \\\n",
       "0  A Bangladesh visa centre will be opened here s...   \n",
       "1  The Eastern Himalaya Travel & Tour Operators’ ...   \n",
       "2  The Eastern Himalaya Travel & Tour Operators’ ...   \n",
       "3  Experienced left-arm pacer Mustafizur Rahman w...   \n",
       "4  On February 15, 13 cyclists began their journe...   \n",
       "\n",
       "                                     content_summary title_translation  \\\n",
       "0  'A proposal to start a border market at Harina...              None   \n",
       "1  Footfall of tourists from Bangladesh had incre...              None   \n",
       "2  Here are the latest developments from India an...              None   \n",
       "3  The accident occurred while they were particip...              None   \n",
       "4  Thirteen cyclists will travel 350km over seven...              None   \n",
       "\n",
       "  content_translation summary translation            author     country  \\\n",
       "0                None                None               PTI  Bangladesh   \n",
       "1                None                None      Avijit Sinha  Bangladesh   \n",
       "2                None                None      Our Web Desk  Bangladesh   \n",
       "3                None                None               PTI  Bangladesh   \n",
       "4                None                None  Debrup Chaudhuri  Bangladesh   \n",
       "\n",
       "     source_localtime bangladesh_localtime  \n",
       "0 2024-02-19 13:58:00  2024-02-19 14:28:00  \n",
       "1 2024-02-19 11:04:00  2024-02-19 11:34:00  \n",
       "2 2024-02-18 17:34:00  2024-02-18 18:04:00  \n",
       "3 2024-02-18 15:48:00  2024-02-18 16:18:00  \n",
       "4 2024-02-17 15:40:00  2024-02-17 16:10:00  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "\n",
    "BASE_URL = \"https://www.telegraphindia.com/\"\n",
    "country = \"bangladesh\"\n",
    "initial_url = f\"{BASE_URL}search?search-term={country}&page=0\"\n",
    "\n",
    "#FINDING THE NUMBER OF PAGES\n",
    "\n",
    "#THE PAGE COUNT STARTS FROM '0' IN THIS WEBSITE \n",
    "\n",
    "#WE WILL TAKE ONE PAGE MORE THAN WHAT IS SCRAPED. THAT'S BECAUSE THE PENULTIMATE PAGE DOESN'T HAVE THE 'NEXT PAGE' BUTTON.\n",
    "\n",
    "#FOR DEMONSTRATION, TAKING ONLY FIRST 13 PAGES. AS OF THE LAST TIME I CHECKED, THE WEBSITE HAD 207 PAGES FOR BANGLADESH NEWS. \n",
    "\n",
    "pages = 0\n",
    "for page in range(0,13):\n",
    "    \n",
    "    initial_url = f\"{BASE_URL}search?search-term={country}&page={page}\"\n",
    "\n",
    "    response = requests.get(initial_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "    pagination_box = soup.find('div', class_ = 'paginationbox mt-80')\n",
    "\n",
    "    next_page_button = pagination_box.find_all('a', {'class':'nxtpvr'})\n",
    "\n",
    "    prev_page_button = pagination_box.find('a', {'class':'nxtpvr', 'aria-label':'Previous'})\n",
    "\n",
    "\n",
    "    if pagination_box:\n",
    "        \n",
    "        if len(next_page_button) > 1:\n",
    "\n",
    "            pages+=1\n",
    "\n",
    "        elif len(next_page_button) == 1:\n",
    "\n",
    "            if next_page_button[0] != prev_page_button:\n",
    "\n",
    "                pages+=1    \n",
    "\n",
    "    else:\n",
    "        \n",
    "        break\n",
    "\n",
    "\n",
    "print(f'Total number of pages taken is {pages}')\n",
    "\n",
    "#THIS LINE SHOULD ONLY BE EXECUTED IF THE AIM IS TO SCRAPE ALL THE PAGES OF THE WEBSITE AVAILABLE FOR BANGLADESH.\n",
    "\n",
    "# pages+=1\n",
    "\n",
    "\n",
    "#SCRAPING ALL THE NEWS LINKS \n",
    "\n",
    "#taking only first page links for testing\n",
    "\n",
    "initial_url = f\"{BASE_URL}search?search-term={country}&page=0\"\n",
    "\n",
    "response = requests.get(initial_url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "news_unordered_list = soup.find_all('ul','storylisting')\n",
    "news_lists = []\n",
    "\n",
    "\n",
    "#THERE ARE TWO UNORDERED LISTS THAT CONTAIN NEWS LINKS IN THIS WEBSITE.\n",
    "\n",
    "#EACH UNORDERED LIST IS FIRST SCRAPED. THE NEWS INSIDE THE UNORDERED LISTS ARE PRESENT AS ORDERED LISTS.\n",
    "\n",
    "#EACH ORDERED LIST IS THEN SCRAPED. AFTER THAT, LINKS FROM EACH LIST IS TAKEN.\n",
    "\n",
    "#AN ISSUE -> FOR SOME REASON, WHEN THE UNORDERED LISTS ARE ITERATED TO APPEND NEWS LISTS, A '-1' IS \n",
    "#APPENDED FOR ALMOST EACH ELEMENT ADDED TO THE 'EACH LIST' VARIABLE. SINCE BeautifulSoup FUNCTIONS DON'T WORK ON INTEGERS, \n",
    "#THE TRY&EXCEPT METHOD IS USED TO REMOVE THE ERROR.\n",
    "\n",
    "for each_list in news_unordered_list:\n",
    "    \n",
    "    each_list.find_all('li')\n",
    "    \n",
    "    for link in each_list:\n",
    "        \n",
    "        try:\n",
    "            news_lists.append(link.find('a').get('href'))\n",
    "            \n",
    "        except AttributeError:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "\n",
    "counter = 0\n",
    "data_list = []\n",
    "\n",
    "\n",
    "#IN THIS WEBSITE, THE WEBPAGES HAVE TWO FORMATS. ONE FORMAT IS THE NORMAL ONE, THE OTHER FORMAT IS PRESENT IN ALL WEBPAGES THAT \n",
    "#CONTAIN THE WORDS 'MY-KOLKATA' IN THE URL. SO, EACH FORMAT IS CONSIDERED FOR SCRAPING PURPOSES. \n",
    "\n",
    "for link in news_lists:\n",
    "    \n",
    "    initial_url = f\"{BASE_URL}{link}\"\n",
    "#     print(initial_url)\n",
    "\n",
    "    response = requests.get(initial_url)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    country = 'Bangladesh'\n",
    "\n",
    "    if 'my-kolkata' not in initial_url:\n",
    "\n",
    "        title_tag = soup.find('div', class_ = 'articletsection pt-40')\n",
    "        website_type = 'normal'\n",
    "\n",
    "    elif 'my-kolkata' in initial_url:\n",
    "\n",
    "        title_tag = soup.find('meta', {'property':'og:title'})\n",
    "        website_type = 'my_kolkata'\n",
    "\n",
    "    else:\n",
    "\n",
    "        website_type = None \n",
    "\n",
    "\n",
    "    if website_type == 'normal':\n",
    "\n",
    "        title = title_tag.find('h1').text \n",
    "        title_translation = 'None'\n",
    "\n",
    "        content_summary_tag = title_tag.find('h2', class_= 'mt-24')\n",
    "        content_summary = content_summary_tag.text if content_summary_tag else 'Content Summary not found'\n",
    "        summary_translation = 'None'\n",
    "\n",
    "\n",
    "        date_info = title_tag.find('div', class_ = 'publishdate mt-32')\n",
    "\n",
    "        if date_info:\n",
    "\n",
    "            date_data = date_info.text.split('Published')[1].strip()\n",
    "\n",
    "            source_localtime = datetime.strptime(date_data,'%d.%m.%y, %I:%M %p')\n",
    "            bangladesh_localtime = source_localtime + timedelta(minutes = 30)\n",
    "\n",
    "            author = date_info.text.split('\\n',2)[1]\n",
    "\n",
    "        else:\n",
    "\n",
    "            date_data = 'Date data not found'\n",
    "\n",
    "        content = []\n",
    "        content_div = soup.find('article', {'id':'contentbox'})\n",
    "\n",
    "        if content_div:\n",
    "\n",
    "            all_paras = content_div.find_all('p') \n",
    "\n",
    "            for para in all_paras:\n",
    "\n",
    "                content.append(para.text)\n",
    "\n",
    "            full_content = ''.join(content)\n",
    "\n",
    "        else:\n",
    "            content = 'Content not found'\n",
    "        \n",
    "        content_translation = 'None'\n",
    "\n",
    "    elif website_type == 'my_kolkata':\n",
    "\n",
    "        title = title_tag.get('content') \n",
    "        title_translation = 'None'\n",
    "        \n",
    "        content_summary_tag = soup.find('meta', {'property':'og:description'})\n",
    "        content_summary = content_summary_tag.get('content') if content_summary_tag else 'Content Summary not found'\n",
    "        summary_translation = 'None'\n",
    "\n",
    "        author_date_tags = soup.find('div',class_ = 'enpublicdate mt24 dfjsb aic')\n",
    "\n",
    "        if author_date_tags:\n",
    "\n",
    "            author = author_date_tags.find('span').text\n",
    "\n",
    "            date_data = author_date_tags.text.split('Published')[1].strip()\n",
    "            source_localtime = datetime.strptime(date_data,'%d.%m.%y, %I:%M %p')\n",
    "\n",
    "            bangladesh_localtime = source_localtime + timedelta(minutes = 30)\n",
    "\n",
    "        else:\n",
    "            date_data = 'Date data not found'\n",
    "\n",
    "        content = []\n",
    "        content_div = soup.find('article', class_ = 'articlecontentbox')\n",
    "\n",
    "        if content_div:\n",
    "\n",
    "            all_paras = content_div.find_all('p') \n",
    "\n",
    "            for para in all_paras:\n",
    "\n",
    "                content.append(para.text)\n",
    "\n",
    "            full_content = ''.join(content)\n",
    "\n",
    "        else:\n",
    "            content = 'Content not found'\n",
    "        \n",
    "        content_translation = 'None'\n",
    "\n",
    "    elif website_type == None:\n",
    "\n",
    "        title = 'Title not found'\n",
    "        content_summary = 'Content Summary not found'\n",
    "        content = 'Content not found'\n",
    "\n",
    "\n",
    "\n",
    "    data_dict = {\n",
    "        \"url\": initial_url,\n",
    "        \"title\": title,\n",
    "        \"content\": full_content,\n",
    "        \"content_summary\": content_summary,\n",
    "        \"title_translation\":title_translation,\n",
    "        \"content_translation\":content_translation,\n",
    "        \"summary translation\":summary_translation,\n",
    "        \"author\": author,\n",
    "        \"country\": country,\n",
    "        'source_localtime': source_localtime,\n",
    "        'bangladesh_localtime': bangladesh_localtime\n",
    "\n",
    "    }\n",
    "\n",
    "    counter+=1\n",
    "\n",
    "\n",
    "    if (full_content != \"Content Not Found\" and content_summary != 'Content summary not found'):\n",
    "\n",
    "            if data_dict not in data_list:\n",
    "                    # Adding to data list\n",
    "                    data_list.append(data_dict)\n",
    "                    print(f'Link {counter} added')\n",
    "    else:\n",
    "            print(f'Link {counter}')\n",
    "            print('Skipped due to missing info.')\n",
    "            \n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "df.head()\n",
    "\n",
    "           \n",
    "\n",
    "csv_filename = f\"{country}_Telegraph_India.csv\"\n",
    "\n",
    "# Checking if the CSV file already exists\n",
    "if os.path.exists(csv_filename):\n",
    "    existing_df = pd.read_csv(csv_filename)\n",
    "    # Merging new and existing dataframe\n",
    "    df = pd.concat([existing_df, pd.DataFrame(data_list)], ignore_index=True)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"bangladesh_localtime\"])  # Converting the \"date\" column to datetime\n",
    "    df = df.drop_duplicates(subset=[\"title\"], keep=\"first\")\n",
    "    df = df.sort_values(by=\"bangladesh_localtime\", ascending=False)  # Sorting the date\n",
    "    df = df.reset_index(drop=True)\n",
    "    df.to_csv(csv_filename, index=False)  \n",
    "else:\n",
    "    # If csv file does not exist, then we create a new CSV file with the scraped data\n",
    "    df = pd.DataFrame(data_list)\n",
    "    df = df.sort_values(by=\"bangladesh_localtime\", ascending=False)\n",
    "    df = df.drop_duplicates(subset=[\"title\"], keep=\"first\")\n",
    "    df = df.reset_index(drop=True)\n",
    "    df.to_csv(csv_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca91066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

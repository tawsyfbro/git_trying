{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b07706c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of article links collected: 60\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains \n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "\n",
    "BASE_URL = 'https://thenewsmill.com/'\n",
    "country = 'bangladesh'\n",
    "initial_url = f'{BASE_URL}?s={country}'\n",
    "\n",
    "#ADDITIONAL NEWS LINKS ARE LOADED BY JUST SCROLLING AND WAITING. NO BUTTONS NEED TO BE PRESSED. \n",
    "\n",
    "#SO WE SCROLL \n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(initial_url)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "top_element = driver.find_element(By.ID, 'main')\n",
    "last_element = driver.find_element(By.ID, 'footer-widgets')\n",
    "\n",
    "number_of_scrolls = 2\n",
    "\n",
    "while number_of_scrolls > 0:\n",
    "\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView();\", last_element)\n",
    "\n",
    "    time.sleep(10)\n",
    "    \n",
    "    number_of_scrolls -= 1\n",
    "    \n",
    "    driver.execute_script(\"arguments[0].scrollIntoView();\", top_element)\n",
    "    \n",
    "    time.sleep(5)\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "main_div = soup.find('div', class_ = 'generate-columns-container')\n",
    "\n",
    "all_news_links = []\n",
    "\n",
    "all_articles = main_div.find_all('article')\n",
    "\n",
    "for each_article in all_articles:\n",
    "    \n",
    "    all_news_links.append(each_article.find('a').get('href'))\n",
    "\n",
    "print(f'Number of article links collected: {len(all_news_links)}')\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f582622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of article links collected: 60\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>content_summary</th>\n",
       "      <th>title_translation</th>\n",
       "      <th>content_translation</th>\n",
       "      <th>summary translation</th>\n",
       "      <th>author</th>\n",
       "      <th>country</th>\n",
       "      <th>source_localtime</th>\n",
       "      <th>bangladesh_localtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://thenewsmill.com/2024/03/it-will-be-a-g...</td>\n",
       "      <td>“It will be a good series”: Shanto opens up on...</td>\n",
       "      <td>Ahead of Bangladesh’s three-match T20I series ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ANI</td>\n",
       "      <td>bangladesh</td>\n",
       "      <td>2024-03-04 02:07:00</td>\n",
       "      <td>2024-03-04 02:37:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://thenewsmill.com/2024/03/more-unemploye...</td>\n",
       "      <td>“More unemployed youths in India than Banglade...</td>\n",
       "      <td>During the Madhya Pradesh leg of the Bharat Jo...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ANI</td>\n",
       "      <td>bangladesh</td>\n",
       "      <td>2024-03-03 13:05:00</td>\n",
       "      <td>2024-03-03 13:35:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://thenewsmill.com/2024/02/sri-lanka-anno...</td>\n",
       "      <td>Sri Lanka announce T20I squad against Banglade...</td>\n",
       "      <td>Sri Lanka Cricket (SLC) on Wednesday announced...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ANI</td>\n",
       "      <td>bangladesh</td>\n",
       "      <td>2024-02-29 00:02:00</td>\n",
       "      <td>2024-02-29 00:32:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://thenewsmill.com/2024/02/bangladesh-app...</td>\n",
       "      <td>Bangladesh appoints new batting, bowling coach...</td>\n",
       "      <td>The Bangladesh Cricket Board (BCB) appointed D...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ANI</td>\n",
       "      <td>bangladesh</td>\n",
       "      <td>2024-02-28 12:03:00</td>\n",
       "      <td>2024-02-28 12:33:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://thenewsmill.com/2024/02/mea-additional...</td>\n",
       "      <td>MEA Additional Secy interacts with 100-member ...</td>\n",
       "      <td>P Kumaran, the Additional Secretary at the Min...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ANI</td>\n",
       "      <td>bangladesh</td>\n",
       "      <td>2024-02-27 20:01:00</td>\n",
       "      <td>2024-02-27 20:31:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://thenewsmill.com/2024/03/it-will-be-a-g...   \n",
       "1  https://thenewsmill.com/2024/03/more-unemploye...   \n",
       "2  https://thenewsmill.com/2024/02/sri-lanka-anno...   \n",
       "3  https://thenewsmill.com/2024/02/bangladesh-app...   \n",
       "4  https://thenewsmill.com/2024/02/mea-additional...   \n",
       "\n",
       "                                               title  \\\n",
       "0  “It will be a good series”: Shanto opens up on...   \n",
       "1  “More unemployed youths in India than Banglade...   \n",
       "2  Sri Lanka announce T20I squad against Banglade...   \n",
       "3  Bangladesh appoints new batting, bowling coach...   \n",
       "4  MEA Additional Secy interacts with 100-member ...   \n",
       "\n",
       "                                             content content_summary  \\\n",
       "0  Ahead of Bangladesh’s three-match T20I series ...            None   \n",
       "1  During the Madhya Pradesh leg of the Bharat Jo...            None   \n",
       "2  Sri Lanka Cricket (SLC) on Wednesday announced...            None   \n",
       "3  The Bangladesh Cricket Board (BCB) appointed D...            None   \n",
       "4  P Kumaran, the Additional Secretary at the Min...            None   \n",
       "\n",
       "  title_translation content_translation summary translation author  \\\n",
       "0              None                None                None    ANI   \n",
       "1              None                None                None    ANI   \n",
       "2              None                None                None    ANI   \n",
       "3              None                None                None    ANI   \n",
       "4              None                None                None    ANI   \n",
       "\n",
       "      country    source_localtime bangladesh_localtime  \n",
       "0  bangladesh 2024-03-04 02:07:00  2024-03-04 02:37:00  \n",
       "1  bangladesh 2024-03-03 13:05:00  2024-03-03 13:35:00  \n",
       "2  bangladesh 2024-02-29 00:02:00  2024-02-29 00:32:00  \n",
       "3  bangladesh 2024-02-28 12:03:00  2024-02-28 12:33:00  \n",
       "4  bangladesh 2024-02-27 20:01:00  2024-02-27 20:31:00  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains \n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "\n",
    "BASE_URL = 'https://thenewsmill.com/'\n",
    "country = 'bangladesh'\n",
    "initial_url = f'{BASE_URL}?s={country}'\n",
    "\n",
    "#ADDITIONAL NEWS LINKS ARE LOADED BY JUST SCROLLING AND WAITING. NO BUTTONS NEED TO BE PRESSED. \n",
    "\n",
    "#SO WE SCROLL FOR A PARTICULAR NUMBER OF TIMES TO LOAD NEWS.\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(initial_url)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "top_element = driver.find_element(By.ID, 'main')\n",
    "last_element = driver.find_element(By.ID, 'footer-widgets')\n",
    "\n",
    "#COLLECTING ALL NEWS URLS\n",
    "\n",
    "number_of_scrolls = 2\n",
    "\n",
    "while number_of_scrolls > 0:\n",
    "\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView();\", last_element)\n",
    "\n",
    "    time.sleep(10)\n",
    "    \n",
    "    number_of_scrolls -= 1\n",
    "    \n",
    "    driver.execute_script(\"arguments[0].scrollIntoView();\", top_element)\n",
    "    \n",
    "    time.sleep(5)\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "main_div = soup.find('div', class_ = 'generate-columns-container')\n",
    "\n",
    "all_news_links = []\n",
    "\n",
    "all_articles = main_div.find_all('article')\n",
    "\n",
    "for each_article in all_articles:\n",
    "    \n",
    "    all_news_links.append(each_article.find('a').get('href'))\n",
    "\n",
    "print(f'Number of article links collected: {len(all_news_links)}')\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "#SCRAPING ALL LINKS\n",
    "counter = 0\n",
    "data_list = []\n",
    "\n",
    "for url in all_news_links[:20]:\n",
    "\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        #TITLE\n",
    "        main_tag = soup.find('h1', class_ = 'gb-headline gb-headline-f593ba8c gb-headline-text')\n",
    "        title = main_tag.text if main_tag else 'Title not found'\n",
    "        title_translation = 'None'\n",
    "\n",
    "        #AUTHOR\n",
    "        author_tag = soup.find('p', class_ = 'gb-headline gb-headline-d90aa5ca gb-headline-text')\n",
    "        author = author_tag.find('a').text if author_tag else 'Author not found'\n",
    "\n",
    "        #DATE\n",
    "\n",
    "        date_info = soup.find('time', class_ = 'entry-date published')\n",
    "\n",
    "        if date_info:\n",
    "\n",
    "            date_data = date_info.get('datetime')\n",
    "\n",
    "            only_date = date_data.split('T')[0]\n",
    "            only_time = date_data.split('T')[1]\n",
    "            time = f\"{only_time.split(':')[0]}:{only_time.split(':')[1]}\"\n",
    "            cleaned_date = f\"{only_date},{time}\"\n",
    "\n",
    "            source_localtime = datetime.strptime(cleaned_date, \"%Y-%m-%d,%H:%M\")\n",
    "            bangladesh_localtime = source_localtime + timedelta(minutes=30)\n",
    "\n",
    "        else:\n",
    "            date_data = 'Date data not found'\n",
    "\n",
    "        content = []\n",
    "\n",
    "        main_container = soup.find('div', class_ = 'gb-container gb-container-1cdc7c96')\n",
    "\n",
    "        if main_container:\n",
    "\n",
    "            all_paras = main_container.find_all('p')\n",
    "\n",
    "            for each_para in all_paras:\n",
    "\n",
    "                content.append(each_para.text)\n",
    "\n",
    "            full_content = ''.join(content)\n",
    "            full_content = full_content.strip()\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            full_content = 'Content not found'\n",
    "\n",
    "        content_translation = 'None'\n",
    "\n",
    "        #NO CONTENT SUMMARIES IN THIS WEBSITE\n",
    "        content_summary = 'None'\n",
    "        summary_translation = 'None'\n",
    "\n",
    "        data_dict = {\n",
    "                \"url\": url,\n",
    "                \"title\": title,\n",
    "                \"content\": full_content,\n",
    "                \"content_summary\": content_summary,\n",
    "                \"title_translation\":title_translation,\n",
    "                \"content_translation\":content_translation,\n",
    "                \"summary translation\":summary_translation,\n",
    "                \"author\": author,\n",
    "                \"country\": country,\n",
    "                'source_localtime': source_localtime,\n",
    "                'bangladesh_localtime': bangladesh_localtime\n",
    "\n",
    "            }\n",
    "\n",
    "        counter+=1\n",
    "\n",
    "\n",
    "        if (full_content != \"Content not found\" or title != 'Title not found'):\n",
    "\n",
    "                    if data_dict not in data_list:\n",
    "                            # Adding to data list\n",
    "                            data_list.append(data_dict)\n",
    "#                             print(f'Link {counter} added')\n",
    "        else:\n",
    "                    print(f'Link {counter}')\n",
    "                    print('Skipped due to missing info.')\n",
    "                    \n",
    "                    \n",
    "df = pd.DataFrame(data_list)\n",
    "df.head()\n",
    "\n",
    "csv_filename = f\"{country}_The_News_Mill.csv\"\n",
    "\n",
    "# Checking if the CSV file already exists\n",
    "if os.path.exists(csv_filename):\n",
    "    existing_df = pd.read_csv(csv_filename)\n",
    "    # Merging new and existing dataframe\n",
    "    df = pd.concat([existing_df, pd.DataFrame(data_list)], ignore_index=True)\n",
    "    df[\"bangladesh_localtime\"] = pd.to_datetime(df[\"bangladesh_localtime\"])  # Converting the \"date\" column to datetime\n",
    "    df = df.drop_duplicates(subset=[\"title\"], keep=\"first\")\n",
    "    df = df.sort_values(by=\"date\", ascending=False)  # Sorting the date\n",
    "    df = df.reset_index(drop=True)\n",
    "    df.to_csv(csv_filename, index=False)  \n",
    "else:\n",
    "    # If csv file does not exist, then we create a new CSV file with the scraped data\n",
    "    df = pd.DataFrame(data_list)\n",
    "    df = df.sort_values(by=\"bangladesh_localtime\", ascending=False)\n",
    "    df = df.drop_duplicates(subset=[\"title\"], keep=\"first\")\n",
    "    df = df.reset_index(drop=True)\n",
    "    df.to_csv(csv_filename, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
